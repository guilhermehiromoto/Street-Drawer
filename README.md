# Street-Drawer


Final project for SCC0251 - Image Processing

![](https://github.com/guilhermehiromoto/Street-Drawer/blob/master/final_result.png?raw=true)

## Authors
11218959 - Guilherme Amaral Hiromoto<br/>
11218855 - Victor Rodrigues Russo<br/>
10892676 - Paulo Matana da Rocha<br/>


## Abstract

This project aims to detect road shapes in urban areas through the segmentation of roads in satellite images using the [Road and Building Detection Dataset](https://www.cs.toronto.edu/~vmnih/data/) and CNN's to train a model that generates maps with the road shapes.

## Example
![](https://github.com/guilhermehiromoto/Street-Drawer/blob/master/example_pdi.png?raw=true)

## Final Report

**Main Objective:** As specified in abstract, this project aims to detect road shapes in urban areas through the segmentation of roads in satellite images using the [Road and Building Detection Dataset](https://www.cs.toronto.edu/~vmnih/data/) and CNN's to train a model that generates maps with the road shapes.

**Input images:** Our input images were obtained from the [Road and Building Detection Dataset](https://www.cs.toronto.edu/~vmnih/data/) using the [Scraping Maps](https://github.com/guilhermehiromoto/Street-Drawer/blob/master/maps/getting_data.ipynb) notebook for the sattelite images and [Scraping Masks](https://github.com/guilhermehiromoto/Street-Drawer/blob/master/masks/getting_data.ipynb) notebook for the masks. As our images had 1500x1500 dimensions we preprocessed then with [Preprocessing](https://github.com/guilhermehiromoto/Street-Drawer/blob/master/preprocessing.ipynb) notebook to fit in our U-net architeture (256x256).

#### Preprocessed data:

![](https://github.com/guilhermehiromoto/Street-Drawer/blob/master/preprocessed_images.png)

**Description of steps:** For this project we are using a model-based methodology for Image Segmentation. We used the Intersection-Over-Union [[1]](https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2) method as metric to train our model, that method consists of calculating an area of overlap between a predicted segmentation and the mask divided by the union area between a predicted segmentation and a mask.<br/>

As loss function we used the binary cross entropy as we are dealing with a binary "classifier":
![](https://github.com/guilhermehiromoto/Street-Drawer/blob/master/binary_cross_entropy.png?raw=true) [[2]](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)

<br/>

For our CNN, we used the U-net architeture to train our [model](https://github.com/guilhermehiromoto/Street-Drawer/blob/master/training_model.ipynb):<br/>
[[3]] (https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47)<br/>
This architeture is divided in two steps, first a **contraction path** that "downsample" the image with convolutions and maxpooling operations to **capture context in the image**, then a **expansion path** that upsample the image with transposed convolution used to **enable precise location**.

**Initial code with first results:** [Preprocessing Data](https://github.com/guilhermehiromoto/Street-Drawer/blob/master/preprocessing.ipynb) and [Model Definition](https://github.com/guilhermehiromoto/Street-Drawer/blob/master/training_model.ipynb)



Initial Prediction:

![](https://github.com/guilhermehiromoto/Street-Drawer/blob/master/initial_predictions.png)<br/>
Obs: Due to the extremely small dataset (only 36 images to elaborate the outline of the project) and an non-optimized architecture, the outputs generated by the model had a result much worse than expected.

## References

[1] [Intersection-Over-Union Metric](https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2)<br/>
[2] [Binary Cross Entropy Loss Function](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)<br/>
[3] [U-net Architeture](https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47)<br/>
